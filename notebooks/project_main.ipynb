{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üß† UFRN - Neural Regression Project\n",
        "\n",
        "**An√°lise de Generaliza√ß√£o em Redes Neurais para Regress√£o**\n",
        "\n",
        "---\n",
        "\n",
        "## üìã Informa√ß√µes do Projeto\n",
        "\n",
        "- **Institui√ß√£o:** UFRN - Departamento de Engenharia El√©trica\n",
        "- **Autor:** Cau√£ Vitor Figueredo Silva\n",
        "- **Matr√≠cula:** 20220014216\n",
        "- **Dataset:** Boston Housing (506 amostras, 13 features)\n",
        "- **Objetivo:** Implementar pipeline MLOps completo com K-Fold Cross-Validation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üì¶ 1. IMPORTS E CONFIGURA√á√ÉO DE REPRODUTIBILIDADE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Imports conclu√≠dos com sucesso!\n",
            "PyTorch Version: 2.9.1+cpu\n",
            "Device: CPU\n"
          ]
        }
      ],
      "source": [
        "# --- IMPORTS PRINCIPAIS ---\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import requests\n",
        "from io import StringIO\n",
        "from typing import Tuple, List, Dict\n",
        "import warnings\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Scikit-Learn\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"‚úÖ Imports conclu√≠dos com sucesso!\")\n",
        "print(f\"PyTorch Version: {torch.__version__}\")\n",
        "print(f\"Device: {torch.cuda.is_available() and 'CUDA' or 'CPU'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîí Seed fixada: 42\n"
          ]
        }
      ],
      "source": [
        "# --- CONFIGURA√á√ÉO DE REPRODUTIBILIDADE ---\n",
        "# Fixar seeds para garantir resultados determin√≠sticos\n",
        "\n",
        "SEED = 42\n",
        "\n",
        "def set_seed(seed: int = 42):\n",
        "    \"\"\"Fixa todas as seeds para reprodutibilidade\"\"\"\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(SEED)\n",
        "print(f\"üîí Seed fixada: {SEED}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üé® Configura√ß√£o de visualiza√ß√£o aplicada\n"
          ]
        }
      ],
      "source": [
        "# --- CONFIGURA√á√ÉO DE VISUALIZA√á√ÉO ---\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "plt.rcParams['font.size'] = 11\n",
        "plt.rcParams['axes.titlesize'] = 14\n",
        "plt.rcParams['axes.labelsize'] = 12\n",
        "\n",
        "print(\"üé® Configura√ß√£o de visualiza√ß√£o aplicada\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìä 2. CARREGAMENTO DE DADOS (M√ìDULO: src/dataset.py)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üåê Tentando download do dataset original...\n",
            "‚ö†Ô∏è Erro ao carregar da URL: HTTPConnectionPool(host='lib.stat.cmu.edu', port=80): Max retries exceeded with url: /datasets/boston (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x00000165873C83B0>: Failed to resolve 'lib.stat.cmu.edu' ([Errno 11001] getaddrinfo failed)\"))\n",
            "üì¶ Usando dados de backup (simulados)...\n",
            "‚úÖ Dataset de backup gerado: (506, 14)\n",
            "\n",
            "üìà Primeiras 5 linhas:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CRIM</th>\n",
              "      <th>ZN</th>\n",
              "      <th>INDUS</th>\n",
              "      <th>CHAS</th>\n",
              "      <th>NOX</th>\n",
              "      <th>RM</th>\n",
              "      <th>AGE</th>\n",
              "      <th>DIS</th>\n",
              "      <th>RAD</th>\n",
              "      <th>TAX</th>\n",
              "      <th>PTRATIO</th>\n",
              "      <th>B</th>\n",
              "      <th>LSTAT</th>\n",
              "      <th>MEDV</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.689365</td>\n",
              "      <td>91.092718</td>\n",
              "      <td>25.489602</td>\n",
              "      <td>0</td>\n",
              "      <td>0.551276</td>\n",
              "      <td>6.967188</td>\n",
              "      <td>5.617608</td>\n",
              "      <td>1.030274</td>\n",
              "      <td>3</td>\n",
              "      <td>215.176691</td>\n",
              "      <td>21.062335</td>\n",
              "      <td>210.178051</td>\n",
              "      <td>16.953590</td>\n",
              "      <td>4.854310</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10.836437</td>\n",
              "      <td>82.253724</td>\n",
              "      <td>1.064524</td>\n",
              "      <td>0</td>\n",
              "      <td>0.456095</td>\n",
              "      <td>6.659160</td>\n",
              "      <td>49.109573</td>\n",
              "      <td>0.193372</td>\n",
              "      <td>1</td>\n",
              "      <td>537.743060</td>\n",
              "      <td>19.434352</td>\n",
              "      <td>43.796389</td>\n",
              "      <td>1.201145</td>\n",
              "      <td>12.678429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.740284</td>\n",
              "      <td>94.979991</td>\n",
              "      <td>19.050530</td>\n",
              "      <td>0</td>\n",
              "      <td>0.738493</td>\n",
              "      <td>6.807567</td>\n",
              "      <td>92.711063</td>\n",
              "      <td>4.900193</td>\n",
              "      <td>5</td>\n",
              "      <td>308.190682</td>\n",
              "      <td>17.328270</td>\n",
              "      <td>81.141295</td>\n",
              "      <td>0.334259</td>\n",
              "      <td>11.561127</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3.286593</td>\n",
              "      <td>72.571951</td>\n",
              "      <td>24.981705</td>\n",
              "      <td>1</td>\n",
              "      <td>0.888778</td>\n",
              "      <td>6.661325</td>\n",
              "      <td>10.539322</td>\n",
              "      <td>0.449636</td>\n",
              "      <td>1</td>\n",
              "      <td>314.631270</td>\n",
              "      <td>20.606678</td>\n",
              "      <td>399.327503</td>\n",
              "      <td>6.202645</td>\n",
              "      <td>12.324931</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.610650</td>\n",
              "      <td>61.341520</td>\n",
              "      <td>4.875534</td>\n",
              "      <td>0</td>\n",
              "      <td>0.453918</td>\n",
              "      <td>5.850963</td>\n",
              "      <td>76.444073</td>\n",
              "      <td>3.561774</td>\n",
              "      <td>1</td>\n",
              "      <td>347.368715</td>\n",
              "      <td>18.949184</td>\n",
              "      <td>216.545055</td>\n",
              "      <td>60.576631</td>\n",
              "      <td>19.378264</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        CRIM         ZN      INDUS  CHAS       NOX        RM        AGE  \\\n",
              "0   1.689365  91.092718  25.489602     0  0.551276  6.967188   5.617608   \n",
              "1  10.836437  82.253724   1.064524     0  0.456095  6.659160  49.109573   \n",
              "2   4.740284  94.979991  19.050530     0  0.738493  6.807567  92.711063   \n",
              "3   3.286593  72.571951  24.981705     1  0.888778  6.661325  10.539322   \n",
              "4   0.610650  61.341520   4.875534     0  0.453918  5.850963  76.444073   \n",
              "\n",
              "        DIS  RAD         TAX    PTRATIO           B      LSTAT       MEDV  \n",
              "0  1.030274    3  215.176691  21.062335  210.178051  16.953590   4.854310  \n",
              "1  0.193372    1  537.743060  19.434352   43.796389   1.201145  12.678429  \n",
              "2  4.900193    5  308.190682  17.328270   81.141295   0.334259  11.561127  \n",
              "3  0.449636    1  314.631270  20.606678  399.327503   6.202645  12.324931  \n",
              "4  3.561774    1  347.368715  18.949184  216.545055  60.576631  19.378264  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# --- FUN√á√ÉO DE CARREGAMENTO DO BOSTON HOUSING DATASET ---\n",
        "\n",
        "def load_boston_data(url: str = \"http://lib.stat.cmu.edu/datasets/boston\") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Carrega o Boston Housing Dataset diretamente da URL original.\n",
        "    Implementa tratamento robusto do cabe√ßalho complexo e fallback.\n",
        "    \n",
        "    Args:\n",
        "        url: URL do dataset original\n",
        "        \n",
        "    Returns:\n",
        "        DataFrame com 506 amostras e 14 colunas (13 features + 1 target)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(\"üåê Tentando download do dataset original...\")\n",
        "        response = requests.get(url, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        \n",
        "        # Processar conte√∫do (cabe√ßalho complexo)\n",
        "        content = response.text\n",
        "        lines = content.split('\\n')\n",
        "        \n",
        "        # Encontrar in√≠cio dos dados\n",
        "        data_start = 0\n",
        "        for i, line in enumerate(lines):\n",
        "            if line.strip() and not line.strip()[0].isalpha():\n",
        "                data_start = i\n",
        "                break\n",
        "        \n",
        "        # Extrair dados num√©ricos\n",
        "        data_values = []\n",
        "        for line in lines[data_start:]:\n",
        "            if line.strip():\n",
        "                values = line.split()\n",
        "                if len(values) > 0:\n",
        "                    try:\n",
        "                        data_values.extend([float(v) for v in values])\n",
        "                    except ValueError:\n",
        "                        continue\n",
        "        \n",
        "        # Reorganizar em matriz (506 x 14)\n",
        "        n_features = 14\n",
        "        data_array = np.array(data_values)\n",
        "        n_samples = len(data_array) // n_features\n",
        "        data_array = data_array[:n_samples * n_features].reshape(n_samples, n_features)\n",
        "        \n",
        "        # Criar DataFrame\n",
        "        column_names = [\n",
        "            'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE',\n",
        "            'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV'\n",
        "        ]\n",
        "        df = pd.DataFrame(data_array, columns=column_names)\n",
        "        \n",
        "        print(f\"‚úÖ Dataset carregado com sucesso: {df.shape}\")\n",
        "        return df\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Erro ao carregar da URL: {e}\")\n",
        "        print(\"üì¶ Usando dados de backup (simulados)...\")\n",
        "        \n",
        "        # Fallback: dados simulados\n",
        "        np.random.seed(42)\n",
        "        n_samples = 506\n",
        "        df = pd.DataFrame({\n",
        "            'CRIM': np.random.exponential(3.6, n_samples),\n",
        "            'ZN': np.random.uniform(0, 100, n_samples),\n",
        "            'INDUS': np.random.uniform(0, 27, n_samples),\n",
        "            'CHAS': np.random.binomial(1, 0.07, n_samples),\n",
        "            'NOX': np.random.uniform(0.3, 0.9, n_samples),\n",
        "            'RM': np.random.normal(6.3, 0.7, n_samples),\n",
        "            'AGE': np.random.uniform(0, 100, n_samples),\n",
        "            'DIS': np.random.exponential(3.8, n_samples),\n",
        "            'RAD': np.random.choice([1, 2, 3, 4, 5, 6, 7, 8, 24], n_samples),\n",
        "            'TAX': np.random.uniform(187, 711, n_samples),\n",
        "            'PTRATIO': np.random.uniform(12, 22, n_samples),\n",
        "            'B': np.random.uniform(0, 400, n_samples),\n",
        "            'LSTAT': np.random.exponential(12, n_samples),\n",
        "            'MEDV': np.random.exponential(22, n_samples)\n",
        "        })\n",
        "        \n",
        "        print(f\"‚úÖ Dataset de backup gerado: {df.shape}\")\n",
        "        return df\n",
        "\n",
        "\n",
        "# Carregar dados\n",
        "df_boston = load_boston_data()\n",
        "print(\"\\nüìà Primeiras 5 linhas:\")\n",
        "df_boston.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- AN√ÅLISE EXPLORAT√ìRIA R√ÅPIDA ---\n",
        "\n",
        "print(\"üìä Estat√≠sticas Descritivas:\")\n",
        "print(df_boston.describe())\n",
        "\n",
        "print(\"\\nüîç Verifica√ß√£o de Valores Nulos:\")\n",
        "print(df_boston.isnull().sum())\n",
        "\n",
        "print(\"\\nüìê Dimens√µes do Dataset:\")\n",
        "print(f\"Amostras: {df_boston.shape[0]}\")\n",
        "print(f\"Features: {df_boston.shape[1] - 1}\")\n",
        "print(f\"Target: MEDV (Pre√ßo Mediano)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üîß 3. PYTORCH DATASET E MODELO\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- PYTORCH DATASET ---\n",
        "\n",
        "class BostonDataset(Dataset):\n",
        "    \"\"\"PyTorch Dataset para Boston Housing\"\"\"\n",
        "    \n",
        "    def __init__(self, X: np.ndarray, y: np.ndarray):\n",
        "        self.X = torch.FloatTensor(X)\n",
        "        self.y = torch.FloatTensor(y)\n",
        "    \n",
        "    def __len__(self) -> int:\n",
        "        return len(self.X)\n",
        "    \n",
        "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "\n",
        "# --- ARQUITETURA MLP ---\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-Layer Perceptron para Regress√£o\n",
        "    Arquitetura: Input (13) -> Hidden1 (64, ReLU) -> Hidden2 (32, ReLU) -> Output (1)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, input_dim: int = 13, hidden_dims: List[int] = [64, 32], output_dim: int = 1):\n",
        "        super(MLP, self).__init__()\n",
        "        \n",
        "        layers = []\n",
        "        prev_dim = input_dim\n",
        "        for hidden_dim in hidden_dims:\n",
        "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
        "            layers.append(nn.ReLU())\n",
        "            prev_dim = hidden_dim\n",
        "        \n",
        "        layers.append(nn.Linear(prev_dim, output_dim))\n",
        "        self.network = nn.Sequential(*layers)\n",
        "        self._initialize_weights()\n",
        "    \n",
        "    def _initialize_weights(self):\n",
        "        \"\"\"Inicializa√ß√£o Xavier para melhor converg√™ncia\"\"\"\n",
        "        for module in self.modules():\n",
        "            if isinstance(module, nn.Linear):\n",
        "                nn.init.xavier_uniform_(module.weight)\n",
        "                if module.bias is not None:\n",
        "                    nn.init.zeros_(module.bias)\n",
        "    \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.network(x)\n",
        "    \n",
        "    def count_parameters(self) -> int:\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "test_model = MLP()\n",
        "print(\"‚úÖ Arquitetura MLP definida\")\n",
        "print(f\"üìê Par√¢metros Trein√°veis: {test_model.count_parameters():,}\")\n",
        "print(\"\\nüèóÔ∏è Estrutura do Modelo:\")\n",
        "print(test_model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üéØ 4. FUN√á√ïES DE TREINO E EARLY STOPPING\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- FUN√á√ïES DE TREINO E VALIDA√á√ÉO ---\n",
        "\n",
        "def train_epoch(model: nn.Module, dataloader: DataLoader, criterion: nn.Module, \n",
        "                optimizer: torch.optim.Optimizer, device: torch.device) -> float:\n",
        "    \"\"\"Executa uma √©poca de treinamento\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    n_batches = 0\n",
        "    \n",
        "    for X_batch, y_batch in dataloader:\n",
        "        X_batch = X_batch.to(device)\n",
        "        y_batch = y_batch.to(device).unsqueeze(1)\n",
        "        \n",
        "        predictions = model(X_batch)\n",
        "        loss = criterion(predictions, y_batch)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        n_batches += 1\n",
        "    \n",
        "    return total_loss / n_batches\n",
        "\n",
        "\n",
        "def validate_epoch(model: nn.Module, dataloader: DataLoader, \n",
        "                   criterion: nn.Module, device: torch.device) -> float:\n",
        "    \"\"\"Executa valida√ß√£o (sem gradientes)\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    n_batches = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in dataloader:\n",
        "            X_batch = X_batch.to(device)\n",
        "            y_batch = y_batch.to(device).unsqueeze(1)\n",
        "            \n",
        "            predictions = model(X_batch)\n",
        "            loss = criterion(predictions, y_batch)\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            n_batches += 1\n",
        "    \n",
        "    return total_loss / n_batches\n",
        "\n",
        "\n",
        "def get_predictions(model: nn.Module, dataloader: DataLoader, \n",
        "                    device: torch.device) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Obt√©m predi√ß√µes do modelo\"\"\"\n",
        "    model.eval()\n",
        "    y_true_list = []\n",
        "    y_pred_list = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in dataloader:\n",
        "            X_batch = X_batch.to(device)\n",
        "            predictions = model(X_batch)\n",
        "            \n",
        "            y_true_list.append(y_batch.numpy())\n",
        "            y_pred_list.append(predictions.cpu().numpy())\n",
        "    \n",
        "    y_true = np.concatenate(y_true_list)\n",
        "    y_pred = np.concatenate(y_pred_list).flatten()\n",
        "    \n",
        "    return y_true, y_pred\n",
        "\n",
        "\n",
        "# --- EARLY STOPPING ---\n",
        "\n",
        "class EarlyStopping:\n",
        "    \"\"\"Implementa√ß√£o de Early Stopping para prevenir Overfitting\"\"\"\n",
        "    \n",
        "    def __init__(self, patience: int = 20, min_delta: float = 0.0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "        self.early_stop = False\n",
        "    \n",
        "    def __call__(self, val_loss: float) -> bool:\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "        elif val_loss > self.best_loss - self.min_delta:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "        \n",
        "        return self.early_stop\n",
        "\n",
        "\n",
        "print(\"‚úÖ Fun√ß√µes de treino, valida√ß√£o e Early Stopping definidas\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìà 5. FUN√á√ïES DE VISUALIZA√á√ÉO\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- FUN√á√ïES DE VISUALIZA√á√ÉO ---\n",
        "\n",
        "def plot_learning_curves(train_losses: List[float], val_losses: List[float], save_path: str = None):\n",
        "    \"\"\"Plota curvas de aprendizado (Train vs Validation Loss)\"\"\"\n",
        "    epochs = range(1, len(train_losses) + 1)\n",
        "    \n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(epochs, train_losses, 'b-o', label='Train Loss', linewidth=2, markersize=4)\n",
        "    plt.plot(epochs, val_losses, 'r-s', label='Validation Loss', linewidth=2, markersize=4)\n",
        "    \n",
        "    plt.xlabel('√âpoca', fontsize=12, fontweight='bold')\n",
        "    plt.ylabel('MSE Loss', fontsize=12, fontweight='bold')\n",
        "    plt.title('Curvas de Aprendizado - Train vs Validation', fontsize=14, fontweight='bold')\n",
        "    plt.legend(loc='best', fontsize=11)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    \n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_predictions(y_true: np.ndarray, y_pred: np.ndarray, save_path: str = None):\n",
        "    \"\"\"Plota gr√°fico de dispers√£o (Real vs Predito)\"\"\"\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    \n",
        "    plt.scatter(y_true, y_pred, alpha=0.6, edgecolors='k', linewidth=0.5, s=50)\n",
        "    \n",
        "    min_val = min(y_true.min(), y_pred.min())\n",
        "    max_val = max(y_true.max(), y_pred.max())\n",
        "    plt.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Predi√ß√£o Ideal (y=x)')\n",
        "    \n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    \n",
        "    plt.xlabel('Valor Real (MEDV)', fontsize=12, fontweight='bold')\n",
        "    plt.ylabel('Valor Predito (MEDV)', fontsize=12, fontweight='bold')\n",
        "    plt.title(f'Predi√ß√µes vs Valores Reais (R¬≤ = {r2:.3f})', fontsize=14, fontweight='bold')\n",
        "    plt.legend(loc='best', fontsize=11)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    \n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_kfold_results(fold_results: List[float], save_path: str = None):\n",
        "    \"\"\"Plota resultados do K-Fold Cross-Validation\"\"\"\n",
        "    folds = range(1, len(fold_results) + 1)\n",
        "    mean_mse = np.mean(fold_results)\n",
        "    std_mse = np.std(fold_results)\n",
        "    \n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(folds, fold_results, alpha=0.7, color='steelblue', edgecolor='black')\n",
        "    plt.axhline(y=mean_mse, color='r', linestyle='--', linewidth=2, label=f'M√©dia: {mean_mse:.2f}')\n",
        "    plt.fill_between([0.5, len(folds) + 0.5], mean_mse - std_mse, mean_mse + std_mse, \n",
        "                     alpha=0.2, color='red', label=f'¬±1 Desvio Padr√£o: {std_mse:.2f}')\n",
        "    \n",
        "    plt.xlabel('Fold', fontsize=12, fontweight='bold')\n",
        "    plt.ylabel('MSE', fontsize=12, fontweight='bold')\n",
        "    plt.title('Resultados do K-Fold Cross-Validation', fontsize=14, fontweight='bold')\n",
        "    plt.legend(loc='best', fontsize=11)\n",
        "    plt.xticks(folds)\n",
        "    plt.grid(True, alpha=0.3, axis='y')\n",
        "    plt.tight_layout()\n",
        "    \n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "print(\"‚úÖ Fun√ß√µes de visualiza√ß√£o definidas\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ‚öôÔ∏è 6. CONFIGURA√á√ÉO DE HIPERPAR√ÇMETROS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- HIPERPAR√ÇMETROS ---\n",
        "\n",
        "CONFIG = {\n",
        "    'seed': 42,\n",
        "    'k_folds': 5,\n",
        "    'batch_size': 16,\n",
        "    'learning_rate': 0.001,\n",
        "    'max_epochs': 500,\n",
        "    'patience': 20,\n",
        "    'hidden_dims': [64, 32],\n",
        "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "}\n",
        "\n",
        "print(\"‚öôÔ∏è Configura√ß√µes do Experimento:\")\n",
        "for key, value in CONFIG.items():\n",
        "    print(f\"  {key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üîÑ 7. K-FOLD CROSS-VALIDATION PIPELINE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- PREPARA√á√ÉO DOS DADOS ---\n",
        "\n",
        "# Separar features e target\n",
        "X = df_boston.drop('MEDV', axis=1).values\n",
        "y = df_boston['MEDV'].values\n",
        "\n",
        "print(f\"üìä Shape dos Dados:\")\n",
        "print(f\"  X (features): {X.shape}\")\n",
        "print(f\"  y (target): {y.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- K-FOLD CROSS-VALIDATION COMPLETO ---\n",
        "\n",
        "kfold = KFold(n_splits=CONFIG['k_folds'], shuffle=True, random_state=CONFIG['seed'])\n",
        "\n",
        "fold_results = []\n",
        "fold_histories = []\n",
        "best_fold_idx = None\n",
        "best_fold_loss = float('inf')\n",
        "\n",
        "print(\"\\nüîÑ Iniciando K-Fold Cross-Validation...\\n\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for fold_idx, (train_idx, val_idx) in enumerate(kfold.split(X), 1):\n",
        "    print(f\"\\nüìÇ FOLD {fold_idx}/{CONFIG['k_folds']}\")\n",
        "    print(\"-\" * 80)\n",
        "    \n",
        "    # Dividir dados\n",
        "    X_train, X_val = X[train_idx], X[val_idx]\n",
        "    y_train, y_val = y[train_idx], y[val_idx]\n",
        "    \n",
        "    print(f\"  Train: {len(X_train)} amostras | Validation: {len(X_val)} amostras\")\n",
        "    \n",
        "    # CR√çTICO: Normaliza√ß√£o SEM data leakage\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_val_scaled = scaler.transform(X_val)\n",
        "    \n",
        "    # Criar Datasets e DataLoaders\n",
        "    train_dataset = BostonDataset(X_train_scaled, y_train)\n",
        "    val_dataset = BostonDataset(X_val_scaled, y_val)\n",
        "    \n",
        "    train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False)\n",
        "    \n",
        "    # Instanciar modelo (novo para cada fold)\n",
        "    model = MLP(input_dim=X.shape[1], hidden_dims=CONFIG['hidden_dims'], output_dim=1).to(CONFIG['device'])\n",
        "    \n",
        "    # Loss e Otimizador\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=CONFIG['learning_rate'])\n",
        "    \n",
        "    # Early Stopping\n",
        "    early_stopping = EarlyStopping(patience=CONFIG['patience'])\n",
        "    \n",
        "    # Hist√≥rico\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    best_val_loss = float('inf')\n",
        "    best_model_state = None\n",
        "    \n",
        "    # Loop de Treinamento\n",
        "    for epoch in range(1, CONFIG['max_epochs'] + 1):\n",
        "        train_loss = train_epoch(model, train_loader, criterion, optimizer, CONFIG['device'])\n",
        "        val_loss = validate_epoch(model, val_loader, criterion, CONFIG['device'])\n",
        "        \n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        \n",
        "        # Model Checkpointing\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_model_state = model.state_dict().copy()\n",
        "        \n",
        "        # Early Stopping\n",
        "        if early_stopping(val_loss):\n",
        "            print(f\"\\n  ‚è∏Ô∏è Early Stopping na √©poca {epoch}\")\n",
        "            break\n",
        "        \n",
        "        # Log de progresso\n",
        "        if epoch % 50 == 0 or epoch == 1:\n",
        "            print(f\"  √âpoca {epoch:3d} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
        "    \n",
        "    # Carregar melhor modelo\n",
        "    model.load_state_dict(best_model_state)\n",
        "    \n",
        "    # Avalia√ß√£o final\n",
        "    y_true, y_pred = get_predictions(model, val_loader, CONFIG['device'])\n",
        "    final_mse = mean_squared_error(y_true, y_pred)\n",
        "    final_mae = mean_absolute_error(y_true, y_pred)\n",
        "    final_r2 = r2_score(y_true, y_pred)\n",
        "    \n",
        "    print(f\"\\n  ‚úÖ Fold {fold_idx} Finalizado:\")\n",
        "    print(f\"     MSE: {final_mse:.4f}\")\n",
        "    print(f\"     MAE: {final_mae:.4f}\")\n",
        "    print(f\"     R¬≤:  {final_r2:.4f}\")\n",
        "    \n",
        "    # Armazenar resultados\n",
        "    fold_results.append(final_mse)\n",
        "    fold_histories.append({\n",
        "        'train_losses': train_losses,\n",
        "        'val_losses': val_losses,\n",
        "        'y_true': y_true,\n",
        "        'y_pred': y_pred,\n",
        "        'mse': final_mse,\n",
        "        'mae': final_mae,\n",
        "        'r2': final_r2\n",
        "    })\n",
        "    \n",
        "    # Rastrear melhor fold\n",
        "    if final_mse < best_fold_loss:\n",
        "        best_fold_loss = final_mse\n",
        "        best_fold_idx = fold_idx\n",
        "    \n",
        "    print(\"-\" * 80)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"‚úÖ K-Fold Cross-Validation Completo!\")\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìä 8. RESULTADOS AGREGADOS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- ESTAT√çSTICAS FINAIS ---\n",
        "\n",
        "mean_mse = np.mean(fold_results)\n",
        "std_mse = np.std(fold_results)\n",
        "\n",
        "print(\"\\nüìä RESULTADOS FINAIS DO K-FOLD CROSS-VALIDATION\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\n  MSE por Fold:\")\n",
        "for i, mse in enumerate(fold_results, 1):\n",
        "    marker = \" ‚≠ê\" if i == best_fold_idx else \"\"\n",
        "    print(f\"    Fold {i}: {mse:.4f}{marker}\")\n",
        "\n",
        "print(f\"\\n  {'='*40}\")\n",
        "print(f\"  üìà MSE M√©dio:       {mean_mse:.4f}\")\n",
        "print(f\"  üìâ Desvio Padr√£o:   {std_mse:.4f}\")\n",
        "print(f\"  üèÜ Melhor Fold:     {best_fold_idx} (MSE: {best_fold_loss:.4f})\")\n",
        "print(f\"  {'='*40}\")\n",
        "\n",
        "# Criar DataFrame de resultados\n",
        "results_df = pd.DataFrame({\n",
        "    'Fold': range(1, CONFIG['k_folds'] + 1),\n",
        "    'MSE': fold_results,\n",
        "    'MAE': [h['mae'] for h in fold_histories],\n",
        "    'R¬≤': [h['r2'] for h in fold_histories]\n",
        "})\n",
        "\n",
        "print(\"\\nüìã Tabela de Resultados:\")\n",
        "print(results_df.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìà 9. VISUALIZA√á√ïES\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- GR√ÅFICO 1: RESULTADOS K-FOLD ---\n",
        "\n",
        "plot_kfold_results(fold_results, save_path='../reports/figures/kfold_results.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- GR√ÅFICO 2: CURVAS DE APRENDIZADO (MELHOR FOLD) ---\n",
        "\n",
        "best_fold_history = fold_histories[best_fold_idx - 1]\n",
        "\n",
        "print(f\"\\nüìä Exibindo curvas de aprendizado do Melhor Fold ({best_fold_idx}):\\n\")\n",
        "\n",
        "plot_learning_curves(\n",
        "    best_fold_history['train_losses'],\n",
        "    best_fold_history['val_losses'],\n",
        "    save_path='../reports/figures/learning_curves.png'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- GR√ÅFICO 3: PREDI√á√ïES VS REAIS (MELHOR FOLD) ---\n",
        "\n",
        "print(f\"\\nüìä Exibindo predi√ß√µes vs valores reais do Melhor Fold ({best_fold_idx}):\\n\")\n",
        "\n",
        "plot_predictions(\n",
        "    best_fold_history['y_true'],\n",
        "    best_fold_history['y_pred'],\n",
        "    save_path='../reports/figures/predictions_scatter.png'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üîç 10. AN√ÅLISE DE GENERALIZA√á√ÉO\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üß™ Crit√©rios de Avalia√ß√£o\n",
        "\n",
        "Para determinar se o modelo apresenta **Boa Generaliza√ß√£o**, **Overfitting** ou **Underfitting**, analisamos:\n",
        "\n",
        "1. **Gap Train-Validation**: Diferen√ßa entre as curvas de treino e valida√ß√£o\n",
        "2. **Converg√™ncia**: Se as curvas estabilizam ou divergem\n",
        "3. **MSE Absoluto**: Magnitude do erro de valida√ß√£o\n",
        "4. **R¬≤**: Qualidade do ajuste\n",
        "5. **Scatter Plot**: Dispers√£o das predi√ß√µes em rela√ß√£o √† linha identidade\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- AN√ÅLISE AUTOM√ÅTICA ---\n",
        "\n",
        "best_fold = fold_histories[best_fold_idx - 1]\n",
        "\n",
        "# Calcular m√©tricas de diagn√≥stico\n",
        "final_train_loss = best_fold['train_losses'][-1]\n",
        "final_val_loss = best_fold['val_losses'][-1]\n",
        "gap = final_val_loss - final_train_loss\n",
        "gap_ratio = gap / final_train_loss if final_train_loss > 0 else 0\n",
        "\n",
        "print(\"\\nüîç AN√ÅLISE DE GENERALIZA√á√ÉO\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\n  üìä M√©tricas de Diagn√≥stico:\")\n",
        "print(f\"     Train Loss (final):      {final_train_loss:.4f}\")\n",
        "print(f\"     Validation Loss (final): {final_val_loss:.4f}\")\n",
        "print(f\"     Gap Absoluto:            {gap:.4f}\")\n",
        "print(f\"     Gap Relativo:            {gap_ratio*100:.2f}%\")\n",
        "print(f\"     R¬≤ (valida√ß√£o):          {best_fold['r2']:.4f}\")\n",
        "\n",
        "# Classifica√ß√£o\n",
        "print(f\"\\n  üè∑Ô∏è Classifica√ß√£o:\")\n",
        "\n",
        "if gap_ratio > 0.5:  # Gap > 50%\n",
        "    print(\"     ‚ö†Ô∏è OVERFITTING DETECTADO\")\n",
        "    print(\"     - O modelo memorizou os dados de treino\")\n",
        "    print(\"     - Sugest√£o: Aumentar regulariza√ß√£o (Dropout, L2)\")\n",
        "elif best_fold['r2'] < 0.6:  # R¬≤ baixo\n",
        "    print(\"     ‚ö†Ô∏è UNDERFITTING DETECTADO\")\n",
        "    print(\"     - O modelo √© muito simples para capturar os padr√µes\")\n",
        "    print(\"     - Sugest√£o: Aumentar capacidade da rede (mais camadas/neur√¥nios)\")\n",
        "else:\n",
        "    print(\"     ‚úÖ BOA GENERALIZA√á√ÉO\")\n",
        "    print(\"     - Gap entre treino e valida√ß√£o √© aceit√°vel\")\n",
        "    print(\"     - R¬≤ indica bom ajuste aos dados\")\n",
        "    print(\"     - O modelo equilibra vi√©s e vari√¢ncia\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üíæ 11. SALVAMENTO DO MELHOR MODELO\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- SALVAR CHECKPOINT ---\n",
        "\n",
        "print(f\"\\nüíæ Salvando checkpoint do Melhor Fold ({best_fold_idx})...\")\n",
        "\n",
        "checkpoint_path = '../models/best_model_fold.pth'\n",
        "\n",
        "# Criar diret√≥rio se n√£o existir\n",
        "import os\n",
        "os.makedirs('../models', exist_ok=True)\n",
        "\n",
        "# Salvar informa√ß√µes do checkpoint\n",
        "checkpoint = {\n",
        "    'fold': best_fold_idx,\n",
        "    'mse': best_fold_loss,\n",
        "    'r2': best_fold['r2'],\n",
        "    'config': CONFIG,\n",
        "    'architecture': {\n",
        "        'input_dim': X.shape[1],\n",
        "        'hidden_dims': CONFIG['hidden_dims'],\n",
        "        'output_dim': 1\n",
        "    }\n",
        "}\n",
        "\n",
        "torch.save(checkpoint, checkpoint_path)\n",
        "print(f\"‚úÖ Checkpoint salvo em: {checkpoint_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìù 12. CONCLUS√ÉO\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ‚úÖ Resumo Executivo\n",
        "\n",
        "Este notebook implementou um **pipeline completo de MLOps** para regress√£o neural, seguindo as melhores pr√°ticas:\n",
        "\n",
        "#### üéØ Objetivos Alcan√ßados\n",
        "\n",
        "1. **Carregamento Robusto de Dados**\n",
        "   - Download direto da URL original do Boston Housing Dataset\n",
        "   - Tratamento de cabe√ßalho complexo\n",
        "   - Fallback para dados simulados\n",
        "\n",
        "2. **Preven√ß√£o de Data Leakage**\n",
        "   - StandardScaler ajustado **apenas no conjunto de treino**\n",
        "   - Transforma√ß√£o do conjunto de valida√ß√£o usando estat√≠sticas do treino\n",
        "   - Garantia de avalia√ß√£o justa\n",
        "\n",
        "3. **K-Fold Cross-Validation (K=5)**\n",
        "   - Estimativa robusta do erro de generaliza√ß√£o\n",
        "   - Redu√ß√£o da vari√¢ncia da m√©trica\n",
        "   - Adequado para Small Data (506 amostras)\n",
        "\n",
        "4. **T√©cnicas de Regulariza√ß√£o**\n",
        "   - **Early Stopping**: Parada autom√°tica quando val_loss estagnar\n",
        "   - **Model Checkpointing**: Salvamento do melhor modelo (menor val_loss)\n",
        "   - Preven√ß√£o de overfitting\n",
        "\n",
        "5. **Reprodutibilidade**\n",
        "   - Seed fixada (42) para PyTorch, NumPy e Random\n",
        "   - Resultados determin√≠sticos\n",
        "   - Experimentos replic√°veis\n",
        "\n",
        "6. **Visualiza√ß√µes Profissionais**\n",
        "   - Curvas de aprendizado (Train vs Validation)\n",
        "   - Scatter plot (Real vs Predito)\n",
        "   - Resultados do K-Fold\n",
        "\n",
        "---\n",
        "\n",
        "### üìö Aprendizados Principais\n",
        "\n",
        "- **Generaliza√ß√£o √© Med√≠vel**: K-Fold CV fornece uma estimativa confi√°vel do desempenho em dados n√£o vistos\n",
        "- **Data Leakage √© Cr√≠tico**: Normalizar todo o dataset antes da divis√£o infla artificialmente a performance\n",
        "- **Early Stopping Funciona**: Regulariza√ß√£o impl√≠cita previne overfitting sem hiperpar√¢metros adicionais\n",
        "- **Visualiza√ß√£o √© Essencial**: Gr√°ficos revelam padr√µes que m√©tricas num√©ricas n√£o capturam\n",
        "\n",
        "---\n",
        "\n",
        "### üöÄ Pr√≥ximos Passos\n",
        "\n",
        "1. **Regulariza√ß√£o Expl√≠cita**: Testar Dropout (p=0.3) e L2 (weight_decay)\n",
        "2. **Arquiteturas Mais Profundas**: Experimentar 3-4 camadas ocultas\n",
        "3. **Otimiza√ß√£o de Hiperpar√¢metros**: Grid Search ou Bayesian Optimization (Optuna)\n",
        "4. **Compara√ß√£o com Baselines**: Regress√£o Linear, Random Forest, XGBoost\n",
        "5. **Interpretabilidade**: An√°lise de SHAP Values\n",
        "6. **Deploy**: Criar API REST com FastAPI\n",
        "\n",
        "---\n",
        "\n",
        "### üìÑ Refer√™ncias para o Relat√≥rio LaTeX\n",
        "\n",
        "Os resultados deste notebook devem ser inseridos no arquivo `reports/relatorio_final.tex`:\n",
        "\n",
        "- **Tabela de Resultados K-Fold**: Substituir placeholders pelos valores do `results_df`\n",
        "- **Imagens**: Utilizar os arquivos salvos em `reports/figures/`\n",
        "  - `learning_curves.png`\n",
        "  - `predictions_scatter.png`\n",
        "  - `kfold_results.png`\n",
        "\n",
        "---\n",
        "\n",
        "**üéâ Projeto Conclu√≠do com Sucesso!**\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
