# An√°lise de Generaliza√ß√£o em Redes Neurais para Regress√£o

## Valida√ß√£o Cruzada K-Fold e Otimiza√ß√£o Bayesiana

---

**Autor:** Cau√£ Vitor Figueredo Silva | **Orientador:** Prof. Dr. Allan de Medeiros Martins

**UFRN - Engenharia El√©trica - ELE 604 | Novembro 2025**

---

# O Desafio do Small Data

**Contexto: Boston Housing Dataset**

* **Volume:** Apenas 506 amostras (Risco cr√≠tico de vi√©s)

* **Dimensionalidade:** 13 features para prever pre√ßo (MEDV)

* **O Problema:** O modelo baseline apresentava **Overfitting severo**, com um gap de **181%** entre treino e valida√ß√£o

---

# An√°lise de Features e Correla√ß√£o

**Entendendo o Alvo (MEDV - Pre√ßo Mediano em k$)**

**Legenda das Features:**
* **CRIM:** Taxa de criminalidade per capita
* **ZN:** Propor√ß√£o de terrenos residenciais zoneados
* **INDUS:** Propor√ß√£o de acres comerciais n√£o-varejo
* **CHAS:** Limita com rio Charles (1=sim, 0=n√£o)
* **NOX:** Concentra√ß√£o de √≥xidos de nitrog√™nio
* **RM:** N√∫mero m√©dio de quartos por habita√ß√£o
* **AGE:** Propor√ß√£o de unidades ocupadas constru√≠das antes de 1940
* **DIS:** Dist√¢ncia ponderada aos centros de emprego
* **RAD:** √çndice de acessibilidade a rodovias radiais
* **TAX:** Taxa de imposto sobre propriedade
* **PTRATIO:** Raz√£o aluno-professor por cidade
* **B:** Propor√ß√£o de negros por cidade
* **LSTAT:** % de popula√ß√£o de baixa renda
* **MEDV:** Pre√ßo mediano de casas (target)

**Vari√°veis Determinantes (Correla√ß√£o de Pearson com MEDV):**

* **LSTAT (-0.74):** Popula√ß√£o de baixa renda (Fator negativo forte)
* **RM (+0.70):** N√∫mero de quartos (Fator positivo forte)
* **PTRATIO (-0.51):** Raz√£o aluno-professor (Fator negativo moderado)

**Interpreta√ß√£o do Mapa de Correla√ß√£o:**
* üî¥ **Vermelho:** Correla√ß√£o positiva (quanto maior a feature, maior o pre√ßo)
* üîµ **Azul:** Correla√ß√£o negativa (quanto maior a feature, menor o pre√ßo)
* ‚ö™ **Branco/Claro:** Correla√ß√£o pr√≥xima de zero (sem rela√ß√£o linear)

> **Ponto de Aten√ß√£o:** A escassez de dados exige valida√ß√£o cruzada rigorosa

**[IMAGEM: `reports/figures/correlation_matrix.png` - Matriz de correla√ß√£o de Pearson destacando LSTAT, RM e PTRATIO]**

---

# Arquitetura da Rede Neural (MLP)

**Topologia e Hiperpar√¢metros Base**

* **Input Layer:** 13 Neur√¥nios

* **Hidden Layers:**
    * Camada 1: 64 Neur√¥nios (ReLU)
    * Camada 2: 32 Neur√¥nios (ReLU)

* **Output Layer:** 1 Neur√¥nio (Linear)

* **Otimizador:** Adam (`lr=0.001`)

* **Loss Function:** Mean Squared Error (MSE)

---

# Metodologia: Pipeline Anti-Leakage

**Garantindo Robustez no Treinamento**

1. **Valida√ß√£o Cruzada:** K-Fold com **K=5** para estimativa real√≠stica de erro

2. **Preven√ß√£o de Vazamento:**
    * `scaler.fit` aplicado **apenas** no conjunto de treino
    * `scaler.transform` aplicado na valida√ß√£o

3. **Objetivo:** Evitar que o modelo "veja" dados de teste atrav√©s da normaliza√ß√£o global

---

# Estrat√©gias de Regulariza√ß√£o

**Combatendo o Gap de 181%**

**Abordagem:** Pipeline combinado aplicado simultaneamente

| T√©cnica | Configura√ß√£o | Fun√ß√£o |
| :--- | :--- | :--- |
| **Dropout** | 30% (0.3) | Evita co-adapta√ß√£o de neur√¥nios durante treino |
| **Weight Decay** | L2 (1e-4) | Penaliza pesos excessivos, promove solu√ß√µes suaves |
| **Early Stopping** | Patience=20 | Interrompe treino quando valida√ß√£o n√£o melhora |
| **Checkpointing** | Best Model | Salva modelo com menor erro de valida√ß√£o |

**Resultado Combinado:** Redu√ß√£o de **~80%** no gap de overfitting (181% ‚Üí 35%)

> **Nota Metodol√≥gica:** As t√©cnicas foram otimizadas em conjunto via Optuna. O impacto individual n√£o foi isolado, pois trabalham sinergicamente.

---

# Otimiza√ß√£o Bayesiana (Optuna)

**Buscando o Estado da Arte (SOTA)**

* **Algoritmo:** TPE Sampler (Parzen Estimator) + Hyperband Pruning

* **Espa√ßo de Busca:**
    * Camadas: 1 a 3
    * Neur√¥nios: 16 a 128
    * Dropout: 0.1 a 0.5

* **Efici√™ncia:** 20 trials conclu√≠dos em **25 minutos** (vs. ~5h de Grid Search)

**[IMAGEM: `reports/figures/optuna_optimization_history.png` - Hist√≥rico de otimiza√ß√£o Optuna]**

---

# Resultados: Redu√ß√£o de Overfitting

**An√°lise Visual das Curvas de Aprendizado**

* **Baseline:** Diverg√™ncia massiva (Gap 181%)

* **Otimizado:** Converg√™ncia suave (Gap 35%)

* **Conclus√£o:** Redu√ß√£o de **80% no overfitting** com a introdu√ß√£o de regulariza√ß√£o e otimiza√ß√£o

**[IMAGEM: `reports/figures/learning_curves.png` (esquerda) e `reports/figures/learning_curves_optimized.png` (direita) - Curvas de aprendizado lado a lado]**

---

# Comparativo de Performance

**M√©tricas (M√©dia 5-Folds)**

| M√©trica | Baseline | Otimizado (Optuna) | Varia√ß√£o |
| :--- | :--- | :--- | :--- |
| **MSE** | 13.47 | **13.02** | -3.3% (Melhor) |
| **R¬≤ (M√©dia)** | 0.852 | **0.857** | +0.5% (Melhor) |
| **R¬≤ (Melhor Fold)** | - | **0.927** (Fold 4) | Potencial m√°ximo |
| **Desvio Padr√£o** | 2.47 | 4.62 | + Vari√¢ncia |

> **Insight:** Embora a m√©dia do R¬≤ tenha se mantido est√°vel (0.857), o modelo atingiu picos de performance superiores (0.927), demonstrando capacidade de aprender padr√µes complexos. O aumento do desvio padr√£o √© um trade-off esperado da otimiza√ß√£o agressiva em *Small Data*, refletindo maior sensibilidade em folds espec√≠ficos.

**[IMAGEM OPCIONAL: `reports/figures/kfold_results.png` - Resultados por fold]**

---

# Valida√ß√£o Final e Generaliza√ß√£o

**Performance em Dados N√£o Vistos**

* **R¬≤ (M√©dia 5-Folds):** **0.857** (Explicamos 85.7% da vari√¢ncia em m√©dia)
* **R¬≤ (Melhor Fold - Fold 4):** **0.927** (92.7% da vari√¢ncia no melhor cen√°rio de valida√ß√£o)

* **Erro M√©dio (RMSE):** ~$3.600 (para im√≥veis de ~$22.500)

* **Diagn√≥stico:** Res√≠duos distribu√≠dos uniformemente, indicando aus√™ncia de vi√©s sistem√°tico significativo. O modelo atingiu R¬≤ de 0.927 no melhor cen√°rio, comprovando alto potencial preditivo quando as condi√ß√µes de treinamento s√£o favor√°veis.

**[IMAGEM: `reports/figures/predictions_scatter_optimized.png` - Scatter plot Real vs Predito com linha de identidade]**

---

# Conclus√µes e Pr√≥ximos Passos

**Principais Conquistas:**

1. Pipeline MLOps robusto e reprodut√≠vel

2. Redu√ß√£o dr√°stica de overfitting (181% ‚Üí 35%)

3. Otimiza√ß√£o eficiente de hiperpar√¢metros

**Limita√ß√µes & Futuro:**

* O **Fold 3** apresentou comportamento outlier (MSE=21.03)

* **Pr√≥ximos passos:** Ensemble de modelos e Feature Engineering (ex: intera√ß√µes n√£o-lineares)

**Reposit√≥rio:** `github.com/takaokensei/ufrn-ele-neural-regression`

---

# Obrigado!

**Perguntas?**

**Cau√£ Vitor Figueredo Silva**

`cauavitorfigueredo@gmail.com`