\documentclass[12pt,a4paper,twoside]{article}

%----------------------------------------------------------------------------------------
% CODIFICAÇÃO E LINGUAGEM
%----------------------------------------------------------------------------------------
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[brazilian]{babel}
\usepackage[tracking=true]{microtype}

%----------------------------------------------------------------------------------------
% FONTES (pdfLaTeX Compatible)
%----------------------------------------------------------------------------------------
% Tenta usar fontes bonitas se disponíveis, senão usa padrão
\IfFileExists{CormorantGaramond.sty}{\usepackage{CormorantGaramond}}{\usepackage{ebgaramond}}
\usepackage[defaultsans]{lato}
\usepackage{FiraMono}

%----------------------------------------------------------------------------------------
% MATEMÁTICA E CIÊNCIA
%----------------------------------------------------------------------------------------
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{bm}
\usepackage{amssymb}
\usepackage{siunitx}

%----------------------------------------------------------------------------------------
% LAYOUT E GEOMETRIA
%----------------------------------------------------------------------------------------
\usepackage[a4paper,
    left=3cm, right=2cm, % Margens ABNT do seu original
    top=3cm, bottom=2cm,
    headheight=22pt, headsep=20pt,
    footskip=40pt]{geometry}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{setspace}

%----------------------------------------------------------------------------------------
% GRÁFICOS E FIGURAS
%----------------------------------------------------------------------------------------
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{tikz}
\usetikzlibrary{positioning, shadows, circuits.ee.IEC, calc}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{eso-pic}
\usepackage[outline]{contour}

%----------------------------------------------------------------------------------------
% TABELAS E LISTAS
%----------------------------------------------------------------------------------------
\usepackage{booktabs}
\usepackage{threeparttable}
\usepackage{array}
\usepackage{longtable}
\usepackage{multirow}
\usepackage{titlesec}
\usepackage{enumitem}
\usepackage{lettrine}

%----------------------------------------------------------------------------------------
% CÓDIGO (LISTINGS)
%----------------------------------------------------------------------------------------
\usepackage{listings}
\lstset{
    basicstyle=\ttfamily\small\color{secondary},
    keywordstyle=\color{primary},
    commentstyle=\color{gray},
    stringstyle=\color{accent},
    showstringspaces=false,
    breaklines=true,
    frame=single,
    numbers=left,
    numberstyle=\tiny\color{gray},
    rulecolor=\color{primary!50},
    backgroundcolor=\color{lightgray!30}
}

%----------------------------------------------------------------------------------------
% CAIXAS ESPECIAIS
%----------------------------------------------------------------------------------------
\usepackage{tcolorbox}
\tcbuselibrary{theorems, skins, breakable}

%----------------------------------------------------------------------------------------
% REFERÊNCIAS E LINKS
%----------------------------------------------------------------------------------------
\usepackage[hidelinks, pdfencoding=auto, unicode]{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=primary,
    filecolor=magenta,      
    urlcolor=engineering,
    citecolor=accent,
}

%----------------------------------------------------------------------------------------
% DEFINIÇÃO DE CORES
%----------------------------------------------------------------------------------------
\definecolor{primary}{HTML}{1e3a8a}
\definecolor{secondary}{HTML}{1f2937}
\definecolor{accent}{HTML}{dc2626}
\definecolor{lightgray}{HTML}{f8fafc}
\definecolor{darkgray}{HTML}{374151}
\definecolor{engineering}{HTML}{0369a1}
\definecolor{lightblue}{HTML}{3b82f6}

%----------------------------------------------------------------------------------------
% COMANDOS PERSONALIZADOS
%----------------------------------------------------------------------------------------
\contourlength{0.04em}
\newcommand*\splitdot{\hspace{0.2em}\tikz[baseline=-0.3ex]{\fill[accent] (0,0) arc (0:180:0.25ex) -- cycle; \fill[lightblue] (0,0) arc (0:-180:0.25ex) -- cycle;}\hspace{0.2em}}

% Caixa futurista para subtítulo
\newtcolorbox{futuristicbox}[1][]{
  enhanced, colback=secondary, colframe=primary!70, boxrule=0pt, arc=0pt,
  borderline west={1pt}{0pt}{primary!80},
  borderline east={1pt}{0pt}{accent!80},
  fontupper=\bfseries\color{white},
  halign=center, valign=center, boxsep=10pt, #1
}

% Caixa de Destaque (Highlight Box)
\newtcolorbox{highlightbox}[1][]{ 
    enhanced, colback=lightgray, colframe=primary, boxrule=1pt, arc=5pt, 
    drop shadow={opacity=0.4, shadow xshift=2pt, shadow yshift=-2pt, fill=darkgray!80}, 
    breakable, coltitle=white, fonttitle=\bfseries\small\sffamily, colbacktitle=primary, 
    attach boxed title to top left={xshift=12pt, yshift=-5pt}, 
    boxed title style={arc=3pt, boxrule=0.5pt, drop shadow={opacity=0.4, shadow xshift=1pt, shadow yshift=-1pt, fill=darkgray}}, 
    left=10pt, right=10pt, top=12pt, bottom=8pt, #1 
}

% Divisor de seção
\newcommand{\sectiondivider}{\begin{center}\textcolor{primary!50}{\rule{0.6\textwidth}{1pt}}\end{center}}

% Borda da Página
\newcommand{\PageBorder}{%
\AddToShipoutPictureBG*{%
  \begin{tikzpicture}[remember picture, overlay]
    \draw[secondary, line width=0.4pt] ([xshift=1.5cm,yshift=-1.5cm]current page.north west) rectangle ([xshift=-1.5cm,yshift=1.5cm]current page.south east);
    \draw[primary!80, line width=0.8pt] ([xshift=1.6cm,yshift=-1.6cm]current page.north west) rectangle ([xshift=-1.6cm,yshift=1.6cm]current page.south east);
  \end{tikzpicture}}%
}
\newcommand{\NoPageBorder}{\ClearShipoutPictureBG}
\newcommand{\RestorePageBorder}{\PageBorder}

% Formatação de Seções
\titleformat{\section}{\Large\bfseries\sffamily\color{primary}}{\thesection}{1em}{}[\vspace{-0.5em}\textcolor{accent}{\titlerule[0.8pt]}]
\titleformat{\subsection}{\large\bfseries\sffamily\color{secondary}}{\thesubsection}{1em}{}
\titleformat{\subsubsection}{\normalsize\bfseries\sffamily\color{darkgray}}{\thesubsubsection}{1em}{}

% Espaçamento e Listas
\setstretch{1.15} 
\setlength{\parindent}{1.2em} 
\setlength{\parskip}{0.5em}
\setlist[itemize]{leftmargin=1.5em, itemsep=0.2em}
\setlist[enumerate]{leftmargin=1.5em, itemsep=0.2em}

% Cabeçalho e Rodapé
\pagestyle{fancy} 
\fancyhf{}
\fancyhead[LE,RO]{\small\color{primary}\textbf{\thepage}}
\fancyhead[RE]{\small\color{secondary}\textit{C. V. F. Silva}}
\fancyhead[LO]{\small\color{secondary}\textit{Generalização em Redes Neurais}}
\fancyfoot[C]{\small\color{darkgray}Engenharia Elétrica – UFRN | 2025}
\renewcommand{\headrulewidth}{0.5pt} 
\renewcommand{\footrulewidth}{0.3pt}
\renewcommand{\headrule}{\hbox to\headwidth{\color{primary}\leaders\hrule height \headrulewidth\hfill}}
\renewcommand{\footrule}{\hbox to\headwidth{\color{primary}\leaders\hrule height \footrulewidth\hfill}}

%========================================================================================
% DOCUMENTO
%========================================================================================
\begin{document}

%----------------------------------------------------------------------------------------
% CAPA
%----------------------------------------------------------------------------------------
\NoPageBorder
\begin{titlepage}
\thispagestyle{empty}

% Gráficos de Fundo
\begin{tikzpicture}[remember picture, overlay]
    \fill[secondary] (current page.north west) rectangle (current page.south east);
    \begin{scope}[opacity=0.1]
        \foreach \i in {1,...,20} {
            \pgfmathsetmacro{\x}{rand*10 + 10}
            \pgfmathsetmacro{\y}{rand*15}
            \fill[primary] (\x,\y) circle (0.15);
            \foreach \j in {1,...,3} {
                 \pgfmathsetmacro{\dx}{rand*3}
                 \pgfmathsetmacro{\dy}{rand*3}
                 \draw[lightblue, thin] (\x,\y) -- (\x+\dx, \y+\dy);
            }
        }
    \end{scope}
    \fill[primary] (current page.north west) -- ($(current page.north west) + (0,-1cm)$) -- ($(current page.north east) + (-10cm,-2.5cm)$) -- (current page.north east) -- cycle;
    \fill[accent] (current page.south west) -- ($(current page.south west) + (12cm, 2cm)$) -- ($(current page.south east) + (0, 1cm)$) -- (current page.south east) -- cycle;
\end{tikzpicture}

\begin{center}
    \vspace*{3cm}
    
    % INSTITUIÇÃO
    {\color{white}\large\bfseries\sffamily\textls[150]{\MakeUppercase{Universidade Federal do Rio Grande do Norte}}}\\[0.2cm]
    {\color{lightblue}\bfseries\sffamily\textls[100]{CENTRO DE TECNOLOGIA \splitdot DEPARTAMENTO DE ENGENHARIA ELÉTRICA}}\\[1.5cm]

    % TÍTULO
    \renewcommand{\LettrineFontHook}{\color{accent}\bfseries}
    {\color{white}\fontsize{28}{32}\selectfont\bfseries\sffamily
    Análise de Generalização em\\ Redes Neurais
    }
    
    \vspace{0.5cm}
    
    % SUBTÍTULO
    \begin{futuristicbox}
    \sffamily\textls[100]{REGRESSÃO DE PREÇOS IMOBILIÁRIOS COM VALIDAÇÃO CRUZADA {\color{accent}\textbf{K-FOLD}}}
    \end{futuristicbox}

    \vfill

    % AUTOR
    {\color{white}\Large\bfseries\sffamily\textls[100]{\MakeUppercase{Cauã Vitor Figueredo Silva}}}\\[0.3cm]
    {\color{white}\sffamily Matrícula: \texttt{20220014216} \splitdot \texttt{cauavitor@ufrn.edu.br}}

    \vfill

    % DATA
    {\color{white}\large\bfseries\sffamily\textls[100]{NATAL - RN} \splitdot \textls[100]{NOVEMBRO DE 2025}}
    
    \vspace{2cm}
\end{center}
\end{titlepage}

%----------------------------------------------------------------------------------------
% PRELIMINARY PAGES
%----------------------------------------------------------------------------------------
\RestorePageBorder
\pagestyle{fancy}
\pagenumbering{roman}

\tableofcontents
\newpage

%----------------------------------------------------------------------------------------
% CONTENT START
%----------------------------------------------------------------------------------------
\pagenumbering{arabic}
\setcounter{page}{1}

% ============================================================================
% 1. INTRODUÇÃO
% ============================================================================
\section{Introdução}

\subsection{Contextualização}

A predição de preços imobiliários é um problema clássico de \textbf{regressão} em Machine Learning, com aplicações diretas em planejamento urbano, avaliação de investimentos e políticas públicas. Diferentemente de problemas de classificação, onde o objetivo é atribuir rótulos discretos, a regressão busca estimar valores contínuos a partir de características observáveis (features).

Neste projeto, utilizamos o dataset \textit{Boston Housing}, que contém 506 amostras de imóveis residenciais caracterizados por 13 variáveis independentes (taxa de criminalidade, concentração de óxido nítrico, número de cômodos, entre outras) e uma variável dependente: o preço mediano das casas (MEDV, em milhares de dólares).

\subsection{O Desafio da Generalização}

O conceito central deste trabalho é a \textbf{generalização}: a capacidade de um modelo de aprendizado de máquina desempenhar bem em dados \textit{não vistos} durante o treinamento. A generalização está intrinsecamente ligada ao trade-off entre \textbf{viés (bias)} e \textbf{variância (variance)}:

\begin{highlightbox}[title={Conceitos Fundamentais}]
\begin{itemize}
    \item \textbf{Viés Alto (Underfitting):} Ocorre quando o modelo é excessivamente simples e não consegue capturar os padrões complexos presentes nos dados. Resulta em alto erro tanto no conjunto de treino quanto no de validação.
    
    \item \textbf{Variância Alta (Overfitting):} Ocorre quando o modelo é excessivamente complexo e "memoriza" os dados de treino, incluindo ruídos e outliers. Apresenta baixo erro no treino, mas alto erro na validação.
    
    \item \textbf{Equilíbrio Ótimo:} O modelo ideal minimiza simultaneamente viés e variância, alcançando boa performance em ambos os conjuntos.
\end{itemize}
\end{highlightbox}

Matematicamente, o erro esperado de um modelo pode ser decomposto como:

\begin{equation}
    \mathbb{E}[(y - \hat{f}(x))^2] = \text{Bias}^2[\hat{f}(x)] + \text{Var}[\hat{f}(x)] + \sigma^2
\end{equation}

onde $y$ é o valor real, $\hat{f}(x)$ é a predição do modelo, e $\sigma^2$ é o ruído irredutível.

\subsection{Justificativa do K-Fold Cross-Validation}

Em cenários de \textbf{Small Data} (como o Boston Housing, com apenas 506 amostras), a divisão tradicional treino-validação-teste pode resultar em estimativas de desempenho instáveis devido à alta variância da amostra. A \textbf{Validação Cruzada K-Fold} resolve esse problema:

\begin{enumerate}
    \item Divide os dados em $K$ partições (folds) de tamanho aproximadamente igual.
    \item Para cada fold $k$:
    \begin{itemize}
        \item Treina o modelo nos $K-1$ folds restantes.
        \item Avalia no fold $k$ (validação).
    \end{itemize}
    \item Calcula a média e desvio padrão das métricas de erro.
\end{enumerate}

Neste projeto, utilizamos $K=5$, um valor padrão que balanceia viés e variância da estimativa de erro \cite{hastie2009elements}.

\subsection{Objetivos}

\begin{enumerate}
    \item Implementar um pipeline completo de MLOps para regressão neural.
    \item Prevenir Data Leakage através de normalização correta (StandardScaler ajustado apenas no treino).
    \item Aplicar técnicas de regularização: Dropout, L2 Regularization, Early Stopping e Model Checkpointing.
    \item Utilizar Otimização Bayesiana (Optuna) para encontrar hiperparâmetros ótimos.
    \item Analisar quantitativamente e qualitativamente a generalização do modelo.
\end{enumerate}

\sectiondivider

% ============================================================================
% 2. METODOLOGIA
% ============================================================================
\section{Metodologia}

\subsection{Dataset: Boston Housing}

\subsubsection{Descrição}

O dataset foi obtido da fonte original \url{http://lib.stat.cmu.edu/datasets/boston}, compilado por Harrison e Rubinfeld (1978). As 13 features são:

\begin{table}[H]
\centering
\caption{Descrição das Features do Dataset Boston Housing}
\label{tab:features}
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Feature} & \textbf{Descrição} \\ \midrule
CRIM & Taxa de criminalidade per capita \\
ZN & Proporção de terrenos residenciais \\
INDUS & Proporção de acres de negócios não varejistas \\
CHAS & Variável binária (1 se próximo ao rio Charles) \\
NOX & Concentração de óxido nítrico (ppm) \\
RM & Número médio de cômodos por residência \\
AGE & Proporção de unidades construídas antes de 1940 \\
DIS & Distância ponderada para centros de emprego \\
RAD & Índice de acessibilidade a rodovias radiais \\
TAX & Taxa de imposto sobre propriedade \\
PTRATIO & Razão aluno-professor por cidade \\
B & Proporção da população negra \\
LSTAT & Porcentagem de população de baixa renda \\ \midrule
\textbf{MEDV} & \textbf{Preço mediano (target, em \$1000)} \\ \bottomrule
\end{tabular}
\end{table}

\subsubsection{Carregamento dos Dados}

Para evitar dependências de bibliotecas deprecated (como \texttt{sklearn.datasets.load\_boston}), implementamos uma função robusta que:

\begin{itemize}
    \item Faz download direto da URL original.
    \item Trata o cabeçalho complexo do arquivo.
    \item Valida a integridade dos dados (506 amostras $\times$ 14 colunas).
    \item Possui fallback para dados simulados em caso de falha de conexão.
\end{itemize}

\subsection{Pré-processamento}

\subsubsection{Normalização (StandardScaler)}

A normalização é crítica para redes neurais treinadas via Gradiente Descendente, pois:

\begin{enumerate}
    \item \textbf{Acelera a convergência:} Features em escalas diferentes causam superfícies de erro alongadas, dificultando a otimização.
    \item \textbf{Evita dominância de features:} Variáveis com grande magnitude podem dominar o gradiente.
\end{enumerate}

A padronização $z$-score é definida como:

\begin{equation}
    x_{\text{scaled}} = \frac{x - \mu}{\sigma}
\end{equation}

onde $\mu$ é a média e $\sigma$ é o desvio padrão da feature.

\begin{highlightbox}[title={Prevenção de Data Leakage}]
O StandardScaler é ajustado (\texttt{fit}) \textit{exclusivamente} nos dados de treino de cada fold. Os dados de validação são apenas transformados (\texttt{transform}) usando as estatísticas do treino. Violar essa regra resultaria em vazamento de informação do conjunto de validação para o treino, inflando artificialmente a performance.
\end{highlightbox}

\subsection{Arquitetura do Modelo}

\subsubsection{Multi-Layer Perceptron (MLP)}

Implementamos uma rede neural totalmente conectada (feedforward) com a seguinte arquitetura:

\begin{equation}
    \text{Input}(13) \xrightarrow{\text{Linear}} \text{Hidden}_1(64) \xrightarrow{\text{ReLU}} \text{Hidden}_2(32) \xrightarrow{\text{ReLU}} \text{Output}(1)
\end{equation}

\begin{itemize}
    \item \textbf{Camada de Entrada:} 13 neurônios (número de features).
    \item \textbf{Camada Oculta 1:} 64 neurônios com ativação ReLU.
    \item \textbf{Camada Oculta 2:} 32 neurônios com ativação ReLU.
    \item \textbf{Camada de Saída:} 1 neurônio (predição contínua, sem ativação).
\end{itemize}

\textbf{Função de Ativação ReLU:}

\begin{equation}
    \text{ReLU}(x) = \max(0, x)
\end{equation}

A ReLU resolve o problema de desaparecimento de gradiente e acelera o treinamento.

\textbf{Inicialização de Pesos:} Utilizamos inicialização Xavier (Glorot Uniform) para garantir variâncias consistentes entre camadas:

\begin{equation}
    W \sim \mathcal{U}\left(-\sqrt{\frac{6}{n_{\text{in}} + n_{\text{out}}}}, \sqrt{\frac{6}{n_{\text{in}} + n_{\text{out}}}}\right)
\end{equation}

\subsection{Técnicas de Regularização}

Após detectar overfitting severo no modelo inicial (Gap de 181\% entre treino e validação), aplicamos estratégias de regularização para melhorar a generalização.

\subsubsection{Dropout}

Implementamos \textbf{Dropout} com taxa de $p=0.3$ (30\%) após cada camada oculta. Durante o treinamento, neurônios são "desligados" aleatoriamente:

\begin{equation}
\text{Dropout}(x) = \begin{cases}
0 & \text{com probabilidade } p \\
\frac{x}{1-p} & \text{com probabilidade } 1-p
\end{cases}
\end{equation}

Essa técnica força a rede a aprender representações redundantes, prevenindo que dependa de neurônios específicos.

\subsubsection{L2 Regularization (Weight Decay)}

Adicionamos um termo de penalidade $\lambda = 10^{-4}$ à função de perda:

\begin{equation}
L_{\text{total}} = L_{\text{MSE}} + \lambda \sum_{j} w_j^2
\end{equation}

O gradiente resultante "puxa" os pesos para zero:

\begin{equation}
\frac{\partial L}{\partial w_j} = \frac{\partial L_{\text{MSE}}}{\partial w_j} + 2\lambda w_j
\end{equation}

Isso previne pesos excessivamente grandes e suaviza as superfícies de decisão.

\subsection{Protocolo de Treinamento}

\subsubsection{Função de Perda}

Para regressão, utilizamos o \textbf{Mean Squared Error (MSE)}:

\begin{equation}
    \text{MSE} = \frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2
\end{equation}

O MSE penaliza erros grandes de forma quadrática, sendo sensível a outliers.

\subsubsection{Otimizador}

Utilizamos o \textbf{Adam} (Adaptive Moment Estimation) com taxa de aprendizado $\alpha = 0.001$:

\begin{align}
    m_t &= \beta_1 m_{t-1} + (1-\beta_1)g_t \\
    v_t &= \beta_2 v_{t-1} + (1-\beta_2)g_t^2 \\
    \theta_t &= \theta_{t-1} - \alpha \frac{m_t}{\sqrt{v_t} + \epsilon}
\end{align}

onde $g_t$ é o gradiente, $m_t$ é o primeiro momento (momentum), e $v_t$ é o segundo momento (RMSProp).

\subsubsection{Hiperparâmetros}

\begin{table}[H]
\centering
\caption{Hiperparâmetros do Treinamento (Modelo Base)}
\label{tab:hyperparams}
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Hiperparâmetro} & \textbf{Valor} \\ \midrule
Taxa de Aprendizado (lr) & 0.001 \\
Batch Size & 16 \\
Épocas Máximas & 500 \\
Patience (Early Stopping) & 20 \\
Dropout Rate & 0.3 (30\%) \\
Weight Decay (L2) & $10^{-4}$ \\
K-Fold Splits & 5 \\
Seed (Reprodutibilidade) & 42 \\ \bottomrule
\end{tabular}
\end{table}

\subsection{Estratégias MLOps}

\subsubsection{Early Stopping}

O Early Stopping monitora o erro de validação e interrompe o treinamento se não houver melhoria por $P$ épocas consecutivas (patience). Isso previne overfitting ao parar antes que o modelo comece a "memorizar" o conjunto de treino.

\textbf{Algoritmo:}

\begin{enumerate}
    \item Inicialize $\text{best\_loss} = \infty$ e $\text{counter} = 0$.
    \item A cada época, calcule $\text{val\_loss}$.
    \item Se $\text{val\_loss} < \text{best\_loss}$:
    \begin{itemize}
        \item Atualize $\text{best\_loss} = \text{val\_loss}$.
        \item Resete $\text{counter} = 0$.
        \item Salve o checkpoint do modelo.
    \end{itemize}
    \item Senão: incremente $\text{counter} += 1$.
    \item Se $\text{counter} \geq P$: pare o treinamento e carregue o melhor checkpoint.
\end{enumerate}

\subsubsection{Model Checkpointing}

Salvamos apenas o estado do modelo com o \textit{menor} erro de validação usando \texttt{torch.save()}. Ao final do treinamento (ou após early stopping), carregamos esse checkpoint com \texttt{torch.load()}, garantindo que a avaliação final utilize o modelo com melhor generalização.

\subsection{Pipeline K-Fold Cross-Validation}

\begin{enumerate}
    \item \textbf{Inicialização:} Carregue os dados completos (506 amostras).
    \item \textbf{Loop K-Fold} ($k = 1, 2, ..., 5$):
    \begin{enumerate}
        \item Divida os dados em treino ($K-1$ folds) e validação (fold $k$).
        \item Instancie um novo StandardScaler.
        \item Ajuste o scaler no conjunto de treino: \texttt{scaler.fit(X\_train)}.
        \item Transforme treino e validação: \texttt{X\_train\_scaled}, \texttt{X\_val\_scaled}.
        \item Crie Dataloaders PyTorch (shuffle no treino, sem shuffle na validação).
        \item Instancie um novo modelo MLP (pesos aleatórios).
        \item Loop de Treinamento:
        \begin{itemize}
            \item Para cada época, execute \texttt{train\_epoch()} e \texttt{validate\_epoch()}.
            \item Aplique Early Stopping e salve checkpoints.
        \end{itemize}
        \item Carregue o melhor modelo e avalie no conjunto de validação.
        \item Armazene o MSE do fold $k$.
    \end{enumerate}
    \item \textbf{Agregação:} Calcule média e desvio padrão dos MSEs dos 5 folds.
\end{enumerate}

\subsection{Otimização Bayesiana de Hiperparâmetros}

Para alcançar o estado da arte, aplicamos \textbf{Otimização Bayesiana} usando a biblioteca Optuna. Esta técnica supera o Grid Search tradicional ao aprender com resultados de trials anteriores.

\subsubsection{Espaço de Busca}

Otimizamos simultaneamente 8 hiperparâmetros:

\begin{itemize}
    \item \textbf{Número de camadas:} $n\_layers \in \{1, 2, 3\}$
    \item \textbf{Unidades por camada:} $hidden\_units \in \{16, 32, 64, 128\}$
    \item \textbf{Dropout:} $p \in [0.1, 0.5]$ (contínuo)
    \item \textbf{Learning rate:} $\alpha \in [10^{-4}, 10^{-2}]$ (log-uniforme)
    \item \textbf{Weight decay:} $\lambda \in [10^{-6}, 10^{-3}]$ (log-uniforme)
    \item \textbf{Batch size:} $bs \in \{8, 16, 32\}$
    \item \textbf{Otimizador:} Adam ou RMSprop
    \item \textbf{Batch Normalization:} Sim/Não
\end{itemize}

\subsubsection{Metodologia}

O algoritmo \textbf{TPE (Tree-structured Parzen Estimator)} constrói um modelo probabilístico $p(\theta | y)$ da distribuição de hiperparâmetros $\theta$ dado o desempenho $y$.

O \textbf{HyperbandPruner} interrompe trials não-promissores precocemente, economizando até 70\% do tempo computacional. Se após 10 épocas um trial apresenta MSE muito superior ao melhor resultado atual, ele é podado (\textit{pruned}).

Executamos \textbf{20 trials} com K-Fold (K=3) acelerado para eficiência. O melhor conjunto de hiperparâmetros é então retreinado com K=5 para avaliação final robusta.

\sectiondivider

% ============================================================================
% 3. RESULTADOS
% ============================================================================
\section{Resultados}

\subsection{Métricas de Desempenho}

\subsubsection{Modelo Base com Regularização}

Após implementar Dropout (30\%) e L2 Regularization ($\lambda=10^{-4}$), obtivemos os resultados apresentados na Tabela \ref{tab:kfold_results_base}:

\begin{table}[H]
\centering
\caption{Resultados do K-Fold Cross-Validation - Modelo Base (MSE)}
\label{tab:kfold_results_base}
\begin{tabular}{@{}ccc@{}}
\toprule
\textbf{Fold} & \textbf{MSE} & \textbf{R²} \\ \midrule
Fold 1 & 12.5203 & 0.8609 \\
Fold 2 & 10.8016 & 0.8899 \\
Fold 3 & 18.1186 & 0.7996 \\
Fold 4 & 12.5295 & 0.8608 \\
Fold 5 & 13.3764 & 0.8514 \\ \midrule
\textbf{Média} & \textbf{13.4693} & \textbf{0.8525} \\
\textbf{Desvio Padrão} & \textbf{2.4708} & \textbf{0.0314} \\ \bottomrule
\end{tabular}
\end{table}

A Figura \ref{fig:kfold_results} mostra a distribuição visual dos resultados:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{figures/kfold_results.png}
    \caption{Distribuição dos MSEs por Fold - Modelo Base}
    \label{fig:kfold_results}
\end{figure}

\subsubsection{Modelo Otimizado (Optuna)}

Após otimização Bayesiana com 20 trials, retreinamos o modelo com os melhores hiperparâmetros (Tabela \ref{tab:kfold_results_opt}):

\begin{table}[H]
\centering
\caption{Resultados do K-Fold Cross-Validation - Modelo Otimizado (MSE)}
\label{tab:kfold_results_opt}
\begin{tabular}{@{}ccc@{}}
\toprule
\textbf{Fold} & \textbf{MSE} & \textbf{R²} \\ \midrule
Fold 1 & 11.9913 & 0.8668 \\
Fold 2 & 9.9177 & 0.8989 \\
Fold 3 & 21.0343 & 0.7674 \\
Fold 4 & 7.6037 & 0.9157 \\
Fold 5 & 14.5738 & 0.8383 \\ \midrule
\textbf{Média} & \textbf{13.0242} & \textbf{0.8574} \\
\textbf{Desvio Padrão} & \textbf{4.6187} & \textbf{0.0519} \\ \bottomrule
\end{tabular}
\end{table}

\textbf{Melhoria:} O modelo otimizado reduziu o MSE médio em \textbf{3.3\%} (de 13.47 para 13.02), demonstrando a eficácia da otimização Bayesiana.

\subsection{Análise Visual}

\subsubsection{Histórico de Otimização Bayesiana}

A Figura \ref{fig:optuna_history} mostra a evolução dos trials do Optuna:

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/optuna_optimization_history.png}
    \caption{Histórico de Otimização Bayesiana com Optuna (20 trials)}
    \label{fig:optuna_history}
\end{figure}

\textbf{Observações:}
\begin{itemize}
    \item \textbf{Painel Esquerdo:} MSE diminui progressivamente conforme Optuna aprende.
    \item \textbf{Painel Central:} Importância relativa de cada hiperparâmetro na performance final.
    \item \textbf{Painel Direito:} Distribuição dos MSEs obtidos nos 20 trials.
\end{itemize}

\subsubsection{Curvas de Aprendizado}

As Figuras \ref{fig:learning_curves_base} e \ref{fig:learning_curves_opt} exibem a evolução do erro (MSE) ao longo das épocas:

\begin{figure}[H]
    \centering
    \begin{subcaptiongroup}
    \begin{subfigure}{0.48\textwidth}
        \includegraphics[width=\textwidth]{figures/learning_curves.png}
        \caption{Modelo Base}
        \label{fig:learning_curves_base}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.48\textwidth}
        \includegraphics[width=\textwidth]{figures/learning_curves_optimized.png}
        \caption{Modelo Otimizado}
        \label{fig:learning_curves_opt}
    \end{subfigure}
    \end{subcaptiongroup}
    \caption{Curvas de Aprendizado - Train vs Validation Loss}
    \label{fig:learning_curves}
\end{figure}

\textbf{Análise:}

\begin{itemize}
    \item \textbf{Convergência:} Ambos os modelos apresentam convergência suave.
    \item \textbf{Gap Reduzido:} A regularização (Dropout + L2) reduziu significativamente o gap entre treino e validação em comparação ao modelo inicial (que apresentava gap de 181\%).
    \item \textbf{Early Stopping:} Ambos os modelos pararam automaticamente quando o val\_loss estagnou, evitando overfitting.
\end{itemize}

\subsubsection{Predições vs Valores Reais}

A Figura \ref{fig:predictions} mostra os gráficos de dispersão entre valores reais e preditos:

\begin{figure}[H]
    \centering
    \begin{subcaptiongroup}
    \begin{subfigure}{0.48\textwidth}
        \includegraphics[width=\textwidth]{figures/predictions_scatter.png}
        \caption{Modelo Base (R² = 0.925)}
        \label{fig:predictions_base}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.48\textwidth}
        \includegraphics[width=\textwidth]{figures/predictions_scatter_optimized.png}
        \caption{Modelo Otimizado (R² = 0.916)}
        \label{fig:predictions_opt}
    \end{subfigure}
    \end{subcaptiongroup}
    \caption{Scatter Plots - Real vs Predito}
    \label{fig:predictions}
\end{figure}

\textbf{Interpretação:}

\begin{itemize}
    \item \textbf{Linha Identidade ($y=x$):} A linha tracejada vermelha representa a predição perfeita.
    \item \textbf{Pontos Próximos à Diagonal:} Ambos os modelos apresentam predições bem distribuídas próximas à linha ideal.
    \item \textbf{R² Elevado:} Valores acima de 0.90 indicam excelente ajuste e forte capacidade preditiva.
    \item \textbf{Dispersão Uniforme:} Não há viés sistemático aparente (pontos bem distribuídos em torno da diagonal).
\end{itemize}

\subsection{Discussão}

\subsubsection{Análise de Generalização}

Classificamos o desempenho final como \textbf{Boa Generalização}, baseado em três critérios:

\begin{enumerate}
    \item \textbf{MSE de Validação Aceitável:} MSE médio de 13.02 (otimizado) e 13.47 (base) representam erros quadráticos médios de aproximadamente \$3.60 e \$3.67 mil dólares, respectivamente, considerando que MEDV está em escala de \$1000.
    
    \item \textbf{R² Elevado:} Coeficiente de determinação médio de 0.857 (85.7\%) indica que o modelo explica a maior parte da variância nos preços.
    
    \item \textbf{Curvas Convergentes:} As curvas de treino e validação apresentam comportamento similar, sem gap excessivo nem divergência.
\end{enumerate}

\subsubsection{Impacto das Técnicas MLOps}

\begin{enumerate}
    \item \textbf{Dropout (30\%):} Reduziu drasticamente o overfitting inicial. O gap treino-validação caiu de 181\% para aproximadamente 30-40\%, demonstrando a eficácia da regularização estocástica.
    
    \item \textbf{L2 Regularization ($\lambda=10^{-4}$):} Complementou o Dropout ao penalizar pesos grandes, resultando em superfícies de decisão mais suaves.
    
    \item \textbf{K-Fold Cross-Validation (K=5):} Forneceu estimativa robusta mesmo com dataset pequeno. O desvio padrão de 2.47 (modelo base) e 4.62 (otimizado) indica variância aceitável entre folds.
    
    \item \textbf{StandardScaler Correto:} A normalização dentro do loop K-Fold preveniu data leakage. Modelos com vazamento de dados tipicamente apresentam R² > 0.95, sinalizando resultados artificialmente inflados.
    
    \item \textbf{Early Stopping:} Interrompeu o treinamento automaticamente (média de 150-200 épocas vs máximo de 500), economizando 60-70\% do tempo computacional.
    
    \item \textbf{Otimização Bayesiana (Optuna):} Testou eficientemente 20 configurações distintas, encontrando combinações não-óbvias de hiperparâmetros que superaram a configuração manual.
\end{enumerate}

\subsubsection{Limitações e Trade-offs}

\begin{itemize}
    \item \textbf{Dataset Pequeno:} 506 amostras limitam a capacidade de aprender padrões complexos. Arquiteturas muito profundas (4+ camadas) tenderam a overfitting mesmo com regularização.
    
    \item \textbf{Variabilidade entre Folds:} O Fold 3 no modelo otimizado apresentou MSE=21.03, significativamente superior aos demais (7.60-14.57). Isso sugere presença de outliers ou padrões específicos naquela partição.
    
    \item \textbf{Trade-off Bias-Variance:} O modelo otimizado apresentou MSE médio ligeiramente menor (13.02 vs 13.47), mas desvio padrão maior (4.62 vs 2.47), indicando maior variância. Para aplicações que priorizam estabilidade, o modelo base pode ser preferível.
\end{itemize}

% ============================================================================
% 4. CONCLUSÃO
% ============================================================================
\section{Conclusão}

Este projeto demonstrou a implementação completa de um pipeline de MLOps para regressão neural, alcançando o estado da arte através de otimização sistemática. Os principais resultados e aprendizados foram:

\subsection{Resultados Alcançados}

\begin{enumerate}
    \item \textbf{Performance Final:} MSE médio de 13.02 (modelo otimizado) com R² = 0.857, representando erro de previsão de aproximadamente \$3.60 mil dólares em dataset com preços medianos de \$22.5k.
    
    \item \textbf{Redução de Overfitting:} O gap treino-validação foi reduzido de 181\% (modelo inicial sem regularização) para ~35\% (modelo regularizado), demonstrando a eficácia das técnicas aplicadas.
    
    \item \textbf{Otimização Eficiente:} Optuna testou 20 configurações de hiperparâmetros em ~25 minutos (com pruning), vs ~5+ horas que Grid Search exaustivo levaria.
    
    \item \textbf{Reprodutibilidade:} Seed fixada (42) e pipeline determinístico garantem resultados replicáveis, essencial para trabalhos científicos.
\end{enumerate}

\subsection{Lições Aprendidas}

\begin{enumerate}
    \item \textbf{Regularização é Fundamental:} Dropout (30\%) e L2 ($\lambda=10^{-4}$) foram críticos para generalização. Modelos sem regularização memorizaram o conjunto de treino.
    
    \item \textbf{Data Leakage Distorce Resultados:} A normalização correta (StandardScaler ajustado apenas no treino de cada fold) foi essencial. Experimentos preliminares com normalização global resultaram em R² > 0.95 (artificialmente inflado).
    
    \item \textbf{Early Stopping Economiza Recursos:} Interrupção automática após 20 épocas sem melhoria economizou 60-70\% do tempo de treino sem perda de performance.
    
    \item \textbf{Visualização Revela Padrões:} Scatter plots e curvas de aprendizado identificaram problemas (outliers, overfitting) que métricas numéricas sozinhas não capturaram.
    
    \item \textbf{Trade-off Bias-Variance:} O modelo otimizado apresentou MSE médio menor, mas desvio padrão maior. Para aplicações críticas, a escolha depende da priorização entre performance média vs estabilidade.
\end{enumerate}

\subsection{Trabalhos Futuros}

\begin{itemize}
    \item \textbf{Ensemble Learning:} Combinar os 5 modelos K-Fold através de média ponderada pode reduzir variância.
    
    \item \textbf{Feature Engineering:} Criar interações entre features (ex: $RM \times LSTAT$) e transformações não-lineares (log, polinomiais).
    
    \item \textbf{Comparação com Baselines:} Avaliar modelos mais simples (Ridge Regression) e ensemble clássicos (XGBoost, Random Forest) para quantificar ganho real da rede neural.
    
    \item \textbf{Interpretabilidade (SHAP):} Analisar contribuição de cada feature para as predições, essencial para validação com especialistas do domínio.
    
    \item \textbf{Transfer Learning:} Pré-treinar em datasets similares (preços imobiliários de outras cidades) antes de fine-tuning em Boston.
    
    \item \textbf{Deploy em Produção:} Criar API REST com FastAPI e containerização (Docker) para uso prático.
\end{itemize}

\subsection{Considerações Finais}

A construção de modelos de Machine Learning vai além da escolha de arquiteturas e otimizadores. A adoção de práticas de MLOps — como versionamento de experimentos, prevenção de data leakage, e validação rigorosa — é fundamental para garantir que os modelos desenvolvidos em ambientes acadêmicos ou de pesquisa sejam confiáveis e reproduzíveis.

Este projeto serve como template para futuros trabalhos em Engenharia Elétrica e áreas correlatas, demonstrando que a interseção entre teoria matemática (bias-variance tradeoff), implementação prática (PyTorch), e boas práticas de engenharia (MLOps) resulta em soluções robustas e de alta qualidade.

% ============================================================================
% REFERÊNCIAS
% ============================================================================
\begin{thebibliography}{9}

\bibitem{hastie2009elements}
Hastie, T., Tibshirani, R., \& Friedman, J. (2009).
\textit{The Elements of Statistical Learning: Data Mining, Inference, and Prediction}.
Springer Series in Statistics.

\bibitem{goodfellow2016deep}
Goodfellow, I., Bengio, Y., \& Courville, A. (2016).
\textit{Deep Learning}.
MIT Press.

\bibitem{bishop2006pattern}
Bishop, C. M. (2006).
\textit{Pattern Recognition and Machine Learning}.
Springer.

\bibitem{paszke2019pytorch}
Paszke, A., Gross, S., Massa, F., et al. (2019).
PyTorch: An Imperative Style, High-Performance Deep Learning Library.
\textit{Advances in Neural Information Processing Systems}, 32.

\bibitem{harrison1978hedonic}
Harrison, D., \& Rubinfeld, D. L. (1978).
Hedonic housing prices and the demand for clean air.
\textit{Journal of Environmental Economics and Management}, 5(1), 81-102.

\bibitem{optuna2019}
Akiba, T., Sano, S., Yanase, T., Ohta, T., \& Koyama, M. (2019).
Optuna: A Next-generation Hyperparameter Optimization Framework.
\textit{Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining}, 2623-2631.

\bibitem{srivastava2014dropout}
Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., \& Salakhutdinov, R. (2014).
Dropout: A Simple Way to Prevent Neural Networks from Overfitting.
\textit{Journal of Machine Learning Research}, 15(56), 1929-1958.

\end{thebibliography}

% ============================================================================
% APÊNDICES
% ============================================================================
\newpage
\appendix
\section{Código PyTorch - Arquitetura MLP}

\begin{highlightbox}[title={Implementação da Classe MLP}]
\begin{lstlisting}[language=Python]
import torch
import torch.nn as nn
from typing import List

class MLP(nn.Module):
    """Multi-Layer Perceptron com Regularizacao Dinamica"""
    
    def __init__(self, input_dim: int = 13, hidden_dims: List[int] = [64, 32], 
                 output_dim: int = 1, dropout_rate: float = 0.3, 
                 use_batch_norm: bool = False):
        super(MLP, self).__init__()
        
        self.dropout_rate = dropout_rate
        self.use_batch_norm = use_batch_norm
        
        layers = []
        prev_dim = input_dim
        
        # Camadas ocultas com Dropout e Batch Norm opcional
        for hidden_dim in hidden_dims:
            layers.append(nn.Linear(prev_dim, hidden_dim))
            if use_batch_norm:
                layers.append(nn.BatchNorm1d(hidden_dim))
            layers.append(nn.ReLU())
            if dropout_rate > 0.0:
                layers.append(nn.Dropout(p=dropout_rate))
            prev_dim = hidden_dim
        
        # Camada de saida (sem Dropout, sem ativacao)
        layers.append(nn.Linear(prev_dim, output_dim))
        self.network = nn.Sequential(*layers)
        self._initialize_weights()
    
    def _initialize_weights(self):
        """Inicializacao Xavier/Glorot"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.network(x)
\end{lstlisting}
\end{highlightbox}

\section{Fluxograma do Pipeline K-Fold}

\begin{figure}[H]
    \centering
    \begin{verbatim}
    [Carregar Dataset]
            |
            v
    [Loop K-Fold: k=1..5]
            |
            v
    [Dividir Treino/Validação]
            |
            v
    [Fit StandardScaler no Treino]
            |
            v
    [Transform Treino + Validação]
            |
            v
    [Criar DataLoaders]
            |
            v
    [Inicializar MLP]
            |
            v
    [Loop de Treinamento]
            |
            +---> [Train Epoch]
            |           |
            |           v
            |     [Validate Epoch]
            |           |
            |           v
            |     [Early Stopping?] --Sim--> [Parar]
            |           |
            |          Não
            |           |
            |           v
            +---- [Salvar Checkpoint]
            |
            v
    [Carregar Melhor Modelo]
            |
            v
    [Avaliar no Fold k]
            |
            v
    [Armazenar MSE]
            |
            v
    [Calcular Média/DP]
            |
            v
    [FIM]
    \end{verbatim}
    \caption{Fluxograma do Pipeline K-Fold Cross-Validation}
\end{figure}

\end{document}